<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>TYM engineering notes</title><link href="/" rel="alternate"></link><link href="/feeds/misc.atom.xml" rel="self"></link><id>/</id><updated>2016-12-22T15:11:04-06:00</updated><entry><title>Stupid bash tricks No. 17: Editing on the fly</title><link href="/stupid-bash-tricks-no-17-editing-on-the-fly.html" rel="alternate"></link><published>2016-12-22T15:11:04-06:00</published><author><name>Thomas</name></author><id>tag:,2016-12-22:stupid-bash-tricks-no-17-editing-on-the-fly.html</id><summary type="html">&lt;p&gt;So, for a while now I've been keeping quick notes in text file in "bullet log" format &amp;mdash; almost a sort of private Twitter for myself. The entry format is simple: a timestamp, the note content, and finally a section marker to set of each entry from the next. Like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;2016-08-10T13:57:46Z

I&amp;#39;m proud to say I&amp;#39;ve officially been riding fixed-gear bike long enough that freewheel feels weird now.

§
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I like to make it easy on myself to add stuff to this file, so I whipped up a quick &amp;amp; dirty little bash script to automate the timestamp and section marker around the entry, and then append the result to the end of the file. For a long time, this has treated STDIN as a heredoc:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;

&lt;span class="nv"&gt;TPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;~/Dropbox/bullet/b0.md  &lt;span class="c1"&gt;# the log file itself&lt;/span&gt;
&lt;span class="c1"&gt;# here&amp;#39;s where it waits for input, then assigns to $CONTENT:&lt;/span&gt;
&lt;span class="nv"&gt;CONTENT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;cat&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# wrap in timestamp and section symbol&lt;/span&gt;
date -u +%FT%TZ &amp;gt;&amp;gt; &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$CONTENT&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;\n&amp;amp;sect;\n&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
atom &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This was fine, but lately I was thinking I'd rather be writing the entry portion in Atom instead of just hanging there until I hit ctl-d. So, just the other day, I revised it to work like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nv"&gt;TPATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;~/Dropbox/bullet/b0.md
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &amp;gt; /tmp/entry
atom --wait /tmp/entry
date -u +%FT%TZ &amp;gt;&amp;gt; &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
cat /tmp/entry &amp;gt;&amp;gt; &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;\n&amp;amp;sect;\n&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
atom &lt;span class="nv"&gt;$TPATH&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, now where I used to get a prompt to type the entry, and Atom window opens a blank temp file instead, and then the script waits for me to save and close that before going on with the rest of its business. It's a little thing, but pretty sweet in practice.&lt;/p&gt;</summary></entry><entry><title>iAdOps and Second-System Effect</title><link href="/iadops-and-second-system-effect.html" rel="alternate"></link><published>2016-11-29T10:15:02-06:00</published><author><name>Thomas</name></author><id>tag:,2016-11-29:iadops-and-second-system-effect.html</id><summary type="html">&lt;p&gt;Some of you have already heard my story about the doomed Campaign Management System project (or, more accurately, series of failed projects) back at Leapfrog Online. (Mark will be able to recall his own direct memories.) Of course, LFO already &lt;em&gt;had&lt;/em&gt; a Campaign Management System in the form of a custom application built in-house, but there was general agreement that the company's business operations and policies had outgrown the assumptions made in the original design. A project to rethink, redesign, and build an entirely new application was a high priority for Tech from before the first day I got there—and despite any number of project reboots, changes in frameworks, and reconstitutions of the team membership, I was there for more than seven years and never saw anything shipped.&lt;/p&gt;
&lt;p&gt;In short, it was a classic case of &lt;a href="http://wiki.c2.com/?SecondSystemEffect"&gt;the Second-System Effect&lt;/a&gt; as described by Fred Brooks in the fifth essay of &lt;em&gt;The Mythical Man-Month&lt;/em&gt;. In particular, the requirement that the new system should do everything that the existing one did, while somehow being "better" in a vaguely defined but comprehensive way, was really impossible to reconcile.&lt;/p&gt;
&lt;p&gt;From the c2 wiki:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If your project is the second system for most of your designers, then it will probably fail outright. If it doesn't fail, it will be bloated, inefficient, and icky.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I am writing this because I have concerns that our current effort to redefine the product design of iAdOps is smelling a lot to me like the restarts of the CMS project that I experienced. The worrying parallels include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We judged an incomplete prototype to be on the wrong track without any actual user testing.&lt;/li&gt;
&lt;li&gt;Executive Management repeated the request that any new iAdOps must include all functionality of the existing tool before it can serve as a replacement.&lt;/li&gt;
&lt;li&gt;The decision to reconsider the fundamentals of our previous working design has (so far) produced more uncertainty about, rather than clearer definition of, the project's goals.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I do not mean to say that entirely disagree with reconsidering the product design and approach, only that the dangers of Second-System Effect are lurking nearby, and I wanted to caution everybody to be on guard against the risks. Complete rewrites of existing software fail more often than they succeed. That we have already been working on this for two years and have returned to product design without shipping any working software suggests that we have already succumbed to the effect to some extent&lt;/p&gt;
&lt;p&gt;So that I not focus entirely on the problem, here are suggested risk-mitigation strategies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Consider whether we can refactor, rather than rewrite, the existing code base.&lt;/li&gt;
&lt;li&gt;Keep goals and scope clear and focused, or make them so.&lt;/li&gt;
&lt;li&gt;Focus on improving, enhancing, or adding one feature at time; we don't have to replace the existing tool all at once.&lt;/li&gt;
&lt;li&gt;Concentrate on specific features, not the general purpose of the application. We can implement features directly, but cannot build abstract ideas as such.&lt;/li&gt;
&lt;li&gt;Apply &lt;a href="http://wiki.c2.com/?YouArentGonnaNeedIt"&gt;YAGNI&lt;/a&gt; liberally.&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Stackdriver custom metrics</title><link href="/stackdriver-custom-metrics.html" rel="alternate"></link><published>2016-11-17T15:38:53-06:00</published><author><name>Thomas</name></author><id>tag:,2016-11-17:stackdriver-custom-metrics.html</id><summary type="html">&lt;p&gt;The last item left to investigate/spike on for Postgres monitoring was setting up custom metrics via the Stackdriver API. This turns out to be fairly easy and quite flexible, and will be my recommendation for how we should proceed with filling in the &lt;a href="http://devblog.ty-m.xyz/postgres-monitoring.html"&gt;remaining monitoring items&lt;/a&gt; that aren't addressed by Stackdriver and stackdriver-agent out of the box.&lt;/p&gt;
&lt;p&gt;For python, connection to the API is managed via the usual &lt;code&gt;discovery&lt;/code&gt; method in the official &lt;code&gt;google-api-python-client&lt;/code&gt; library (and of course there are libraries for JS, Go, etc. as well).&lt;/p&gt;
&lt;p&gt;Creation of a custom metric just requires provisioning a metric description and associating that to a metric type name. The basic structure of a metric is shown in the following python dictionary example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;metrics_descriptor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;custom_metric_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;labels&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;key&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;environment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;valueType&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;STRING&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;description&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;An arbitrary measurement&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;metricKind&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;metric_kind&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;valueType&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;INT64&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;unit&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;items&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;description&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;An arbitrary measurement.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;displayName&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Custom Metric&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The value in the &lt;code&gt;"type"&lt;/code&gt; node is just an arbitrary value in the &lt;code&gt;custom.googleapis.com&lt;/code&gt; namespace, for example 'custom.googleapis.com/postgres/connection_status'. The &lt;a href="https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.metricDescriptors#MetricKind"&gt;&lt;code&gt;metricKind&lt;/code&gt;&lt;/a&gt; is generally one of 'GAUGE' or 'CUMULATIVE'. The &lt;code&gt;"labels"&lt;/code&gt; provide a set of descriptive tags used to distinguish among specific instances of a metric type (e.g., a label might identify which Postgres instance a particular time series was coming from).&lt;/p&gt;
&lt;p&gt;For writing data to a custom metric, we use the &lt;a href="https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.timeSeries/create"&gt;&lt;code&gt;timeSeries.create&lt;/code&gt;&lt;/a&gt; method. The structure looks complicated, but essentially we simply post a timestamp and data value to a given custom metric object for a resource. For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;timeseries_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;metric&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;custom.googleapis.com/trial_metric&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;labels&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;pg_instance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;warehouse&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;resource&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;gce_instance&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;labels&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;instance_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;gke-warehouse-provisioning-1-fde62020-node-l8ba&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;&amp;#39;zone&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;us-central1-f&amp;#39;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;points&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;interval&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;startTime&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;endTime&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;now&lt;/span&gt;
            &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;value&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;boolValue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note it is also possible to create a new custom metric and start posting data to it in a single step.&lt;/p&gt;
&lt;p&gt;The recommendation for resource is to use the physical layer on which the app being measured resides, although it is possible to set that to &lt;code&gt;gke_container&lt;/code&gt;. &lt;a href="https://cloud.google.com/monitoring/api/resources"&gt;The docs have a full list of monitored resource types&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Using custom metrics for Postgres monitoring seems like the best bet. It may be possible to get something like &lt;a href="https://bucardo.org/wiki/Check_postgres"&gt;check_postgres&lt;/a&gt; to produce output that can be read and posted to a custom metric. Or, since the number of checks to be added is relatively small compared to everything included in check_postgres, it might be easier to submit our own queries to Postgres using psycopg2 or whatever client, and then provision a TimeSeries object from that.&lt;/p&gt;
&lt;p&gt;Although basic client connection to the Stackdriver API is provided by the standard &lt;code&gt;google-api-python-client&lt;/code&gt; library, it will probably be worthwhile to provide a small helper module to generalize the task of creating and sending data to custom metrics from Postgres checks.&lt;/p&gt;</summary></entry><entry><title>Postgres Monitoring Trial Findings</title><link href="/postgres-monitoring-trial-findings.html" rel="alternate"></link><published>2016-11-08T12:07:58-06:00</published><author><name>Thomas</name></author><id>tag:,2016-11-08:postgres-monitoring-trial-findings.html</id><summary type="html">&lt;p&gt;In order to supplement the &lt;a href="http://devblog.ty-m.xyz/postgres-monitoring.html"&gt;monitoring available in Stackdriver&lt;/a&gt;, I took a look at a number of options, as noted in &lt;a href="https://trello.com/c/1pWdNWVM"&gt;this card&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;TL;DR&lt;/h2&gt;
&lt;p&gt;I'm not overjoyed about any of the solutions I tried, although OPM could do the job if we can't come up with anything else.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://no0p.github.io/pgantenna/"&gt;pgantenna&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This looked promising, based on the style of the project homepage, and the  helpful documentation. There was already a &lt;a href="https://hub.docker.com/r/no0p/pgantenna/"&gt;docker image&lt;/a&gt; available, so getting this up and running was simple.&lt;/p&gt;
&lt;p&gt;The app gathers data from a Postgres extension called &lt;a href="http://no0p.github.io/pgsampler/"&gt;pgsampler&lt;/a&gt;. Building this and installing it a postgres container was easy and straightforward: &lt;code&gt;make &amp;amp;&amp;amp; make install&lt;/code&gt; and then add to &lt;code&gt;postgresql.conf&lt;/code&gt; as a shared_preload_libraries value.&lt;/p&gt;
&lt;p&gt;Unfortunately, the pgantenna server just served a default Apache page on port 80, and I was unable to noodle out what the URL for the pgantenna app. At this point I noticed that the project hasn't had a new commit in 2 years, so it was unlikely that asking the developer for help would do much, and I abandoned the effort here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: doesn't actually work.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://opm.io/"&gt;OPM&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This turns out to be a vended distribution of Nagios, with a custom UI wrapper, and with config and commands for Postgres already included (although still requiring further customization), and its own &lt;a href="https://github.com/OPMDG/check_pgactivity"&gt;Nagios plugin&lt;/a&gt; (an alternative to the more widely-used &lt;a href="https://bucardo.org/wiki/Check_postgres"&gt;&lt;code&gt;check_postgres&lt;/code&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;h4&gt;Pros&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Can do everything Nagios does (threshold-based altering + some metric graphs) without so much of the Nagios setup. Indeed, it actually &lt;em&gt;is&lt;/em&gt; Nagios, with some of the Postgres-specific stuff taken care of already.&lt;/li&gt;
&lt;li&gt;Nicer looking front-end than Nagios for monitoring&lt;/li&gt;
&lt;li&gt;&lt;code&gt;check_pgactivity&lt;/code&gt; does a lot of the more useful things in &lt;code&gt;check_postgres&lt;/code&gt; without as much complexity/bloat. Certainly covers what we're looking for.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Cons&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;It's still Nagios, therefore an ugly beast&lt;/li&gt;
&lt;li&gt;Feels like both more and less than what I'm looking for (i.e., too heavyweight but also not great).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Verdict&lt;/strong&gt;: Could be workable, but I'm not sold&lt;/p&gt;
&lt;h4&gt;Next Steps&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Research/investigate Stackdriver custom metrics as a way to do this monitoring&lt;ul&gt;
&lt;li&gt;Would have the advantage of integration with GCP account and the monitoring we are already using it for&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Draft more detailed plan for "rolling our own" solution&lt;ul&gt;
&lt;li&gt;Unlike my earlier post, I would be interested not in setting up a Graphite server, but a CLI thing like &lt;a href="https://github.com/zalando/pg_view"&gt;pg_view&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Although, we'd still have to solve the problem of active/push altering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Postgres Monitoring</title><link href="/postgres-monitoring.html" rel="alternate"></link><published>2016-10-11T18:04:22+00:00</published><author><name>Thomas</name></author><id>tag:,2016-10-11:postgres-monitoring.html</id><summary type="html">&lt;h2&gt;What we have in place&lt;/h2&gt;
&lt;h4&gt;Monitors&lt;/h4&gt;
&lt;p&gt;Via &lt;a href="https://app.google.stackdriver.com/services/postgresql"&gt;Stackdriver&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Connections (count): Number of connections to PostgreSQL.&lt;/li&gt;
&lt;li&gt;Disk Usage (byte): Number of bytes currently being used on disk.&lt;/li&gt;
&lt;li&gt;Commits (count/s): Number of commits per second.&lt;/li&gt;
&lt;li&gt;Rollbacks (count/s): Number of rollbacks per second.&lt;/li&gt;
&lt;li&gt;Heap Blocks Read Rate (count/s): Number of blocks read from the heap.&lt;/li&gt;
&lt;li&gt;Heap Cache Hit Rate (count/s): Number of blocks read directly out of the cache.&lt;/li&gt;
&lt;li&gt;Index Blocks Read Rate (count/s): Number of blocks read from the index.&lt;/li&gt;
&lt;li&gt;Index Cache Hit Rate (count/s): Number of index blocks read directly out of the cache.&lt;/li&gt;
&lt;li&gt;Toast Blocks Read Rate (count/s): Number of reads from the toast blocks.&lt;/li&gt;
&lt;li&gt;Toast Cache Hit Rate (count/s): Number of toast blocks read directly out of the cache.&lt;/li&gt;
&lt;li&gt;Toast Index Blocks Read Rate (count/s): Number of blocks read from the toast index.&lt;/li&gt;
&lt;li&gt;Toast Index Cache Hit Rate (count/s): Number of toast index blocks read directly out of the cache.&lt;/li&gt;
&lt;li&gt;Operations [delete, insert, update, heap only update] (count/s): Number of rows [deleted, inserted, updated, heap only updated] in the db.&lt;/li&gt;
&lt;li&gt;Dead Tuples (count): Number of tuples that are dead in the db.&lt;/li&gt;
&lt;li&gt;Live Tuples (count): Number of tuples that are live in the db.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plus CPU, RAM, disk, and network traffic stats on the host&lt;/p&gt;
&lt;p&gt;All of the above are simply being passively monitored; we don't have any threshold-based alerting on any metrics.&lt;/p&gt;
&lt;h4&gt;Alerting policies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Uptime Check Health on mart.pgdata.xyz fails&lt;/li&gt;
&lt;li&gt;Uptime Check Health on warehouse.pgdata.xyz fails&lt;/li&gt;
&lt;li&gt;CPU Usage is absent for greater than 5 minutes from warehouse.pgdata.xyz&lt;/li&gt;
&lt;li&gt;CPU Usage is absent for greater than 10 minutes from api-postgres&lt;/li&gt;
&lt;li&gt;CPU Usage is absent for greater than 5 minutes from mart-postgres&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When raised, these all send notice to the #data channel in Slack, and push notification to TYM via the Google Could Console app.&lt;/p&gt;
&lt;h2&gt;What we should still add&lt;/h2&gt;
&lt;h4&gt;Things to monitor&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Count of database locks&lt;/li&gt;
&lt;li&gt;Duration of queries (avg, longest-running)&lt;/li&gt;
&lt;li&gt;Count and duration of idle-in-transaction connections&lt;/li&gt;
&lt;li&gt;Last checkpoint&lt;/li&gt;
&lt;li&gt;Actual uptime based on database connection (existing checks use ping or CPU metric availability)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Things to alert on&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;blocking locks (at all)&lt;/li&gt;
&lt;li&gt;exclusive locks (threshold-based)&lt;/li&gt;
&lt;li&gt;connections waiting for a lock (threshold-based)&lt;/li&gt;
&lt;li&gt;long-running queries&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Some options (needs research)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Custom metrics via Stackdriver API&lt;/li&gt;
&lt;li&gt;&lt;a href="https://no0p.github.io/pgantenna/"&gt;pgantenna&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://opm.io/"&gt;OPM&lt;/a&gt; (Don't miss &lt;a href="https://github.com/yuhuman/OPM-Docker"&gt;Dockerized&lt;/a&gt; version)&lt;/li&gt;
&lt;li&gt;Roll our own check_postgres, Nagios, collectd, graphite server&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Stackdriver</title><link href="/stackdriver.html" rel="alternate"></link><published>2016-07-19T03:31:03+00:00</published><author><name>Thomas</name></author><id>tag:,2016-07-19:stackdriver.html</id><summary type="html">&lt;p&gt;I've been studying &lt;a href="https://cloud.google.com/stackdriver/"&gt;Stackdriver Monitoring&lt;/a&gt; for Google Cloud, and setting up simple alerts and metric dashboards. I wrote up my findings on &lt;a href="https://github.com/tym-oao/tym-oao.github.io/wiki/Stackdriver-Monitoring"&gt;this wiki page&lt;/a&gt;.&lt;/p&gt;</summary></entry><entry><title>Using pg_service.conf with Luigi</title><link href="/using-pg_serviceconf-with-luigi.html" rel="alternate"></link><published>2016-07-14T17:42:15+00:00</published><author><name>Thomas</name></author><id>tag:,2016-07-14:using-pg_serviceconf-with-luigi.html</id><summary type="html">&lt;p&gt;I was thinking about the pg_service.conf post I wrote yesterday, and wondering how to follow that practice in &lt;a href="http://luigi.readthedocs.io/"&gt;Luigi&lt;/a&gt;. It's a little tricky, because &lt;a href="http://luigi.readthedocs.io/en/stable/api/luigi.postgres.html"&gt;&lt;code&gt;luigi.postgres&lt;/code&gt;&lt;/a&gt; implements instances of &lt;code&gt;luigi.contrib.rdbms&lt;/code&gt; abstract classes, and therefore expects all the abstract properties (i.e., host, database, etc.) to be specified. So, when I tried simply overriding the &lt;code&gt;connect()&lt;/code&gt; method of &lt;code&gt;PostgresTarget&lt;/code&gt; I was getting TypeErrors because those abstract methods weren't implemented.&lt;/p&gt;
&lt;p&gt;I didn't want to build this up from a generic Luigi &lt;code&gt;Task&lt;/code&gt;, because &lt;code&gt;luigi.postgres&lt;/code&gt; and &lt;code&gt;luigi.contrib.rdbms&lt;/code&gt; have a lot of really useful stuff already implemented that I would have had to repeat for myself. In the end, I found that I could just set the &lt;code&gt;rdms&lt;/code&gt; properties to &lt;code&gt;None&lt;/code&gt; defaults and then ignore them, while overriding &lt;code&gt;PostgresTarget.connect()&lt;/code&gt; the way I wanted to. The resulting template tasks, along with a couple of simple examples of subclassing them for specific jobs are in &lt;a href="https://gist.github.com/yagermadden/d515f0b1dde2c4cdd6c192e08bb33e00"&gt;this Gist&lt;/a&gt; which is also shown below:&lt;/p&gt;
&lt;div class="gist"&gt;
    &lt;script src='https://gist.github.com/d515f0b1dde2c4cdd6c192e08bb33e00.js'&gt;&lt;/script&gt;
    &lt;noscript&gt;
        &lt;pre&gt;&lt;code&gt;import luigi
import luigi.postgres
import psycopg2
import os

testdata = ([99, 'My fake plants died because I did not pretend to water'
             ' them.'],
            [100, 'I always arrive late at the office, '
            'but I make up for it by leaving early.'],
            [101, u'∩ ∪'])


class PgServiceTarget(luigi.postgres.PostgresTarget):
    """
    Target for a resource in PostgreSQL, overriding the standard PostgresTarget
    to use a pg_service.conf service name to make the connection instead of
    separate connection params
    """
    def __init__(self, service, update_id, table):
        """
        Args:
            service (str): the name of a service defined in local
                pg_service.conf file
            update_id (str): An identifier for this data set
        """
        self.service = service
        self.update_id = update_id
        self.table = table

    def connect(self):
        """
        Get a psycopg2 connection object to the database where the table is.
        """
        connection = psycopg2.connect(
            service=self.service)
        connection.set_client_encoding('utf-8')
        return connection


class PgServiceQuery(luigi.postgres.PostgresQuery):
    """
    Template task for querying a PostgreSQL database, with standard output
    method overridden to return a PgServiceTarget
    """
    # Handle the properties expected by abstract class rdbms.Query
    host = None
    database = None
    user = None
    password = None

    def output(self):
        return PgServiceTarget(service=self.service, table=self.table, update_id=self.update_id)


class PgCopyToTable(luigi.postgres.CopyToTable):
    """
    Template task for inserting a data set into Postgres via PgServiceTarget
    """
    # Handle the properties expected by abstract class rdbms.Query
    host = None
    database = None
    user = None
    password = None

    def output(self):
        return PgServiceTarget(service=self.service, table=self.table, update_id=self.update_id)


class PgExampleQuery(PgServiceQuery):
    env_service = os.getenv('PGSERVICE', 'local')
    service = luigi.Parameter(default=env_service)
    table = 'whoa'
    query = 'create table whoa (trival_id serial, whoa_txt text)'


class PgExampleLoad(PgCopyToTable):
    env_service = os.getenv('PGSERVICE', 'local')
    service = luigi.Parameter(default=env_service)
    table = 'whoa'
    columns = [("trival_id", "INT"),
               ("description", "TEXT")]

    def rows(self):
        for record in testdata:
            yield record
&lt;/code&gt;&lt;/pre&gt;
    &lt;/noscript&gt;
&lt;/div&gt;</summary></entry><entry><title>pg_service + psycopg2 = ❤️</title><link href="/pg_service-psycopg2.html" rel="alternate"></link><published>2016-07-13T16:13:30+00:00</published><author><name>Thomas</name></author><id>tag:,2016-07-13:pg_service-psycopg2.html</id><summary type="html">&lt;p&gt;A couple of days ago I learned about the &lt;a href="https://www.postgresql.org/docs/current/static/libpq-pgservice.html"&gt;pg_service.conf file&lt;/a&gt; in Postgres, and posted about it to my teammates in Slack. It lets you store connection parameters under single service names, using square-bracked "INI file" format. This makes connecting to frequently-used hosts and databases much easier. For example, with a &lt;code&gt;pg_service.conf&lt;/code&gt; like...&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[mydb]&lt;/span&gt;
&lt;span class="na"&gt;host&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;somehost&lt;/span&gt;
&lt;span class="na"&gt;port&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;5433&lt;/span&gt;
&lt;span class="na"&gt;user&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;someuser&lt;/span&gt;
&lt;span class="na"&gt;dbname&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;mydatabase&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;...then you can go &lt;code&gt;PGSERVICE=mydb psql&lt;/code&gt; at the command line, and Bob's your uncle. (This is extra handy if you have the &lt;code&gt;PGPASSWORD&lt;/code&gt; envar set to the password for &lt;code&gt;someuser&lt;/code&gt;.) Instead of always setting the PGSERVICE variable, I also export that to my most-often-used service (the warehouse instance, in my case) in my &lt;code&gt;.bashrc&lt;/code&gt;. Another way I made this easier on myself is with by adding a simple alias-like function (I used a function because aliases can't take arguments):&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pg&lt;span class="o"&gt;()&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="nv"&gt;PGSERVICE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt; psql

&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;The other noteworthy thing here, is that the defined services work at the libpq level, which means that any native postgresql driver (for instance psycopg2 😉) can use them as well.&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;psycopg2&lt;/span&gt;

&lt;span class="n"&gt;conn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;psycopg2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;connect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;service&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mydb&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;password&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;secret&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# or set PGPASSWORD&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;I made a more complete example in &lt;a href="https://gist.github.com/tym-oao/33baf67bb332cebc4b20f7211dbedf59"&gt;this gist&lt;/a&gt;. I suggest we should use this manner of provisioning database connections as a matter of policy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ADD&lt;/code&gt; a pg_service.conf file to your Docker image (likely better: provide for one to be mounted in at run time).&lt;/li&gt;
&lt;li&gt;Use Kubernetes secret to set &lt;code&gt;PGPASSWORD&lt;/code&gt; in the environment&lt;/li&gt;
&lt;li&gt;Pass the service name to the psycopg2 (or whichever driver) connection method&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It just makes sense.&lt;/p&gt;</summary></entry><entry><title>Corrupt database disk issue at GCE</title><link href="/corrupt-database-disk-issue-at-gce.html" rel="alternate"></link><published>2016-06-29T22:54:20+00:00</published><author><name>Thomas</name></author><id>tag:,2016-06-29:corrupt-database-disk-issue-at-gce.html</id><summary type="html">&lt;p&gt;Posting this here mainly in case this issue happens to come up again. I've noticed several cases in recent cloud console Notification Activity in which repeated disk-attachment errors were reported. In those cases I didn't notice any database service outages, but this still seems like something that could end up not being an isolated incident.&lt;/p&gt;
&lt;h3&gt;Problem&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Between 22:10 and 22:20 on 2016-06-29, the Google cloud console Activity record shows that the persistent disks mounted to nodes in the &lt;code&gt;warehouse-provisioning-1&lt;/code&gt; k8s cluster detached themselves from the instances they had been attached to.&lt;/li&gt;
&lt;li&gt;I say "detached themselves" because neither myself nor anyone at OAO executed any k8s commands to explicitly detach them. The root cause for the detachment is uncertain.&lt;/li&gt;
&lt;li&gt;The timing of &lt;a href="https://status.cloud.google.com/incident/compute/16011"&gt;this SSD latency incident incident&lt;/a&gt; from the GCE &lt;a href="https://status.cloud.google.com"&gt;status page&lt;/a&gt; seems to coincide with when these detachments happened. I imagine that disks might have had to be detached and reattached as part of whatever Google engineers did to resolve the issue, but that's a guess on my part.&lt;/li&gt;
&lt;li&gt;In any case, while the disks were also automatically reattached, the abrupt disconnection left them in an inconsistent state, so that Postgres instances which use these disk mounts for storage were unable to start due to I/O errors.&lt;/li&gt;
&lt;li&gt;Result: &lt;code&gt;warehouse-postgres&lt;/code&gt; and &lt;code&gt;mart-postgres&lt;/code&gt; instances were down since roughly 22:19.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Resolution&lt;/h3&gt;
&lt;p&gt;Basically, to get things working again, I had to &lt;code&gt;fsck&lt;/code&gt; the affected disks, then detach them from the instances they were attached to with a &lt;code&gt;gcloud&lt;/code&gt; command. So, for example, in the case of the disk &lt;code&gt;mart-data-disk&lt;/code&gt; attached to &lt;code&gt;warehouse-provisioning-1-fde62020-node-8n9r&lt;/code&gt; I had to:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh warehouse-provisioning-1-fde62020-node-8n9r &lt;span class="se"&gt;\&lt;/span&gt;
sudo fsck.ext4 -v -y /dev/disk/by-id/google-mart-data-disk
gcloud compute instances detach-disk &lt;span class="se"&gt;\&lt;/span&gt;
gke-warehouse-provisioning-1-fde62020-node-8n9r --disk mart-data-disk
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Then restart the &lt;code&gt;mart-postgres&lt;/code&gt; as usual with &lt;code&gt;kubectl create -f mart-postgres.yaml&lt;/code&gt;.&lt;/p&gt;</summary></entry><entry><title>Self-referential TODO</title><link href="/self-referential-todo.html" rel="alternate"></link><published>2016-06-23T07:24:35-05:00</published><author><name>Thomas</name></author><id>tag:,2016-06-23:self-referential-todo.html</id><summary type="html">&lt;p&gt;&lt;s&gt;While I want to keeping the styling and build of this page brutally simple, one thing I should consider adding is &lt;a href="http://pygments.org/"&gt;pygments&lt;/a&gt; or similar for code syntax highlighting.&lt;/s&gt; &lt;em&gt;(UPDATE: done!)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;s&gt;Also, look into why &lt;a href="https://github.com/yagermadden/straplessdown"&gt;straplessdown&lt;/a&gt; is stripping inline HTML from the markdown source section. 😕 &lt;/s&gt; &lt;em&gt;(UPDATE: wontfix; using Pelican now instead.)&lt;/em&gt;&lt;/p&gt;</summary></entry><entry><title>Distinct line items from DFP line item service</title><link href="/distinct-line-items-from-dfp-line-item-service.html" rel="alternate"></link><published>2016-05-18T00:00:00-05:00</published><author><name>Thomas</name></author><id>tag:,2016-05-18:distinct-line-items-from-dfp-line-item-service.html</id><summary type="html">&lt;p&gt;Over time, the table &lt;code&gt;warehouse_hold.hold_dfp_line_item&lt;/code&gt; has accumulated many rows for each line item. In most cases this is due to changes in the line item configuration or status; it is also due to overlapping attempts to bring the data up-to-date. In any case, Analytics is interested only in the most recent record for each item. As noted in &lt;a href="https://trello.com/c/ygfrMXJr/615-warehouse-etl-investigate-and-fix-line-item-discrepancies-between-dfp-and-hold-dfp-line-item-table"&gt;Trello&lt;/a&gt;, I created a new &lt;code&gt;hold_dfp_line_item_distinct&lt;/code&gt; table, to include one record per line_item_id, based on maximum &lt;code&gt;last_modified_date_time&lt;/code&gt; and &lt;code&gt;created_at&lt;/code&gt; values.&lt;/p&gt;
&lt;p&gt;Oddly (says I), while a CTE defined as&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;select&lt;/span&gt; &lt;span class="k"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;last_modified_date_time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;last_modified_date_time&lt;/span&gt;
     &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;created_at&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;created_at&lt;/span&gt;
     &lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line_item_id&lt;/span&gt;
  &lt;span class="k"&gt;from&lt;/span&gt; &lt;span class="n"&gt;hold_dfp_line_item&lt;/span&gt;
 &lt;span class="k"&gt;group&lt;/span&gt; &lt;span class="k"&gt;by&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;has the expected number of rows, joining that on all three columns back to the main table resulted in a small number (39 out of about a million) of duplicate rows.&lt;/p&gt;
&lt;p&gt;I have been able to resolve the matter by using &lt;code&gt;select distinct&lt;/code&gt;  in the main query, but I don't understand why it should be necessary.&lt;/p&gt;
&lt;p&gt;Also, as we update with new pulls from the line item service, we'll begin to have multiple rows per line item again, and if this simplified table is desirable in Hold, we'll have to keep refreshing it. Therefore, we will eventually find out whether the duplication of the same line item at the same created_at timestamp was a one-time glitch, or something ongoing. In the former case, we should remove the &lt;code&gt;distinct&lt;/code&gt; from the query, for performance reasons. This note is here as a reminder&lt;/p&gt;</summary></entry><entry><title>Handling fact updates</title><link href="/handling-fact-updates.html" rel="alternate"></link><published>2016-05-11T07:24:35-05:00</published><author><name>Thomas</name></author><id>tag:,2016-05-11:handling-fact-updates.html</id><summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href="http://decisionworks.com/2010/12/design-tip-130-accumulating-snapshots-for-complex-workflows/"&gt;Accumulating snapshot&lt;/a&gt; fact table pattern sounded attractive, but we cannot apply it to our case. It is a way of handling late-arriving facts, but supposes access to the transactional workflow milestone data.&lt;/li&gt;
&lt;li&gt;In our case, DFP is already returning us pre-aggregated snapshot data; they restate the snapshot data over time as more atomic data accumulates on their end.&lt;/li&gt;
&lt;li&gt;For this reason, the best approach will be to reload over a reasonable window to keep our fact table records current with the latest stated values from DFP&lt;/li&gt;
&lt;li&gt;Exact number of days in the window TBD based on analysis of recent re-pulls of data we have already loaded.&lt;/li&gt;
&lt;li&gt;Best guess as of now: 14-day reload window (requires further investigation to rule out first-of-month being exceptional)&lt;/li&gt;
&lt;/ul&gt;</summary></entry><entry><title>Slowly-changing dimension handling for dim_line_item</title><link href="/slowly-changing-dimension-handling-for-dim_line_item.html" rel="alternate"></link><published>2016-05-09T00:00:00-05:00</published><author><name>Thomas</name></author><id>tag:,2016-05-09:slowly-changing-dimension-handling-for-dim_line_item.html</id><summary type="html">&lt;ul&gt;
&lt;li&gt;Hold table load is somewhat incremental already: all modified items get added on to the hold table (not really distinguished from prior versions, but we can use &lt;code&gt;last_modified_date_time&lt;/code&gt; for that)&lt;/li&gt;
&lt;li&gt;So, pull for &lt;code&gt;dim_line_item&lt;/code&gt; update begins by taking all records with most recent &lt;code&gt;created_at&lt;/code&gt; timestamps from &lt;code&gt;hold_dfp_line_item&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;These can be inserted directly to &lt;code&gt;dim_line_item&lt;/code&gt;, with &lt;code&gt;current_date&lt;/code&gt; as &lt;code&gt;effective_dt&lt;/code&gt; and &lt;code&gt;created_at&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Then find &lt;code&gt;line_item_code&lt;/code&gt; values that match newly-inserted rows, with prior &lt;code&gt;created_at&lt;/code&gt; and NULL &lt;code&gt;effective_end_dt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;update &lt;code&gt;effective_end_dt&lt;/code&gt; of those rows with current_date&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: treat line_item_code as the fixed SCD key; treat change in any other column as requiring a new record.&lt;/li&gt;
&lt;/ul&gt;</summary></entry></feed>